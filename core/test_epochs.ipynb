{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d93489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from matplotlib import cm\n",
    "from torch import nn, optim\n",
    "from __future__ import annotations\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d1e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\scientific_research\\sniffnet\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Юля\\.cache\\kagglehub\\datasets\\bloodlaac\\products-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"bloodlaac/products-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88fb022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd7a4a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84d0507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_dir = f\"{path}\\products_dataset\"\n",
    "\n",
    "FOOD = [\n",
    "    'FreshApple', 'FreshBanana', 'FreshMango', 'FreshOrange', 'FreshStrawberry',\n",
    "    'RottenApple', 'RottenBanana', 'RottenMango', 'RottenOrange', 'RottenStrawberry',\n",
    "    'FreshBellpepper', 'FreshCarrot', 'FreshCucumber', 'FreshPotato', 'FreshTomato',\n",
    "    'RottenBellpepper', 'RottenCarrot', 'RottenCucumber', 'RottenPotato', 'RottenTomato'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d554c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(food_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9608c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledDataset():\n",
    "    def __init__(self, food_dir: Path, food_classes: list[str], transform=None) -> LabeledDataset:\n",
    "        self.food_dir = food_dir\n",
    "        self.food_classes = food_classes\n",
    "        self.transform = transform\n",
    "        self.images_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for cls_name in food_classes:\n",
    "            class_path = Path(food_dir)\n",
    "            class_path /= cls_name\n",
    "\n",
    "            for image_name in class_path.iterdir():\n",
    "                image_path = class_path / image_name\n",
    "                self.images_paths.append(image_path)\n",
    "                self.labels.append(food_classes.index(cls_name))\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images_paths)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        image = Image.open(self.images_paths[index]).convert(\"RGB\")\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9388bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomCrop([200, 200]),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "    #transforms.ColorJitter(),\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.3, 0.3)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.485, 0.456, 0.406]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd2509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_dataset = LabeledDataset(food_dir, FOOD, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a0fbf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(food_dataset)\n",
    "n_train = int(0.6 * total)\n",
    "n_val   = int(0.2 * total)\n",
    "n_test  = total - n_train - n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abc8ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(food_dataset, [n_train, n_val, n_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc58507",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders = []\n",
    "\n",
    "for batch_size in (16, 32, 64):\n",
    "    train_dataloaders.append(\n",
    "        DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=7,\n",
    "            pin_memory=True,  # TODO: fix\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "559fc21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8284a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Create basic unit of ResNet.\n",
    "    Consists of two convolutional layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            stride: int = 1,\n",
    "            downsampling=None\n",
    "        ) -> Block:\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1\n",
    "        )\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,  # TODO: Replace with padding=\"same\"\n",
    "            padding=1\n",
    "        )\n",
    "        self.downsampling = downsampling\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        input = x\n",
    "\n",
    "        pred = self.batch_norm(self.conv1(x))\n",
    "        pred = self.relu(pred)\n",
    "        pred = self.batch_norm(self.conv2(pred))\n",
    "        \n",
    "        if self.downsampling is not None:\n",
    "            input = self.downsampling(x)\n",
    "        \n",
    "        pred += input\n",
    "        pred = self.relu(pred)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6471c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Build model ResNet and return prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, blocks_num_list: list[int]) -> ResNet:\n",
    "        \"\"\"\n",
    "        ResNet init.\n",
    "\n",
    "        Arguments:\n",
    "        blocks_num_list -- number of basic blocks for each layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64  # Default number of channels for first layer. Mutable!\n",
    "\n",
    "        # Reduce resolution of picture by 2\n",
    "        # 224 -> 112\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=64,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3\n",
    "        )\n",
    "        self.batch_norm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 112 -> 56\n",
    "\n",
    "        self.layer1 = self.create_layer(  # Default stride. No resolution reduction.\n",
    "            out_channels=64,\n",
    "            num_blocks=blocks_num_list[0]\n",
    "        )\n",
    "        self.layer2 = self.create_layer(  # Resolution reduction. 56 -> 28\n",
    "            out_channels=128,\n",
    "            num_blocks=blocks_num_list[1],\n",
    "            stride=2\n",
    "        )\n",
    "        self.layer3 = self.create_layer(  # Resolution reduction. 28 -> 14\n",
    "            out_channels=256,\n",
    "            num_blocks=blocks_num_list[2],\n",
    "            stride=2\n",
    "        )\n",
    "        self.layer4 = self.create_layer(  # Resolution reduction. 14 -> 7\n",
    "            out_channels=512,\n",
    "            num_blocks=blocks_num_list[3],\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, 20)\n",
    "    \n",
    "    def create_layer(\n",
    "            self,\n",
    "            out_channels: int,\n",
    "            num_blocks: int,\n",
    "            stride: int = 1\n",
    "        ) -> nn.Sequential:\n",
    "        \"\"\"\n",
    "        Create ResNet layer.\n",
    "\n",
    "        out_channels -- number of output channels per block\n",
    "        num_blocks -- number of blocks per layer\n",
    "        stride -- step of filter in conv layer (default 1)\n",
    "        \"\"\"\n",
    "        downsampling = None\n",
    "\n",
    "        if stride != 1:\n",
    "            downsampling = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=self.in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        blocks: list[Block] = []\n",
    "        \n",
    "        blocks.append(Block(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=out_channels,\n",
    "            stride=stride,\n",
    "            downsampling=downsampling\n",
    "        ))\n",
    "\n",
    "        self.in_channels = out_channels\n",
    "\n",
    "        for _ in range(num_blocks - 1):\n",
    "            blocks.append(Block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pred = self.batch_norm(self.conv1(x))\n",
    "        pred = self.relu(pred)\n",
    "        pred = self.pooling(pred)\n",
    "\n",
    "        pred = self.layer1(pred)\n",
    "        pred = self.layer2(pred)\n",
    "        pred = self.layer3(pred)\n",
    "        pred = self.layer4(pred)\n",
    "\n",
    "        pred = self.avgpool(pred)\n",
    "        pred = torch.flatten(pred, 1)\n",
    "        pred = self.fc(pred)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a762451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(\n",
    "        epochs: int,\n",
    "        train_history: list,\n",
    "        val_history: list,\n",
    "        optimizer_name: str,\n",
    "        label: str\n",
    "    ):\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 10))\n",
    "    ax1.plot(np.arange(1, epochs + 1), train_history, label=label)\n",
    "    ax2.plot(np.arange(1, epochs + 1), val_history, label=label)\n",
    "\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend(loc='lower right')\n",
    "        ax.grid(True)\n",
    "\n",
    "    ax1.set_title(f'{optimizer_name} Training accuracy')\n",
    "    ax2.set_title(f'{optimizer_name} Validation accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5cf7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        images, labels = batch[0], batch[1]\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(images)\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "\n",
    "        total += len(pred)\n",
    "        correct += (pred == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "760dc372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, train_loader, val_loader, optimizer, epochs=10):\n",
    "    train_acc, val_acc = [], []\n",
    "    model.train()\n",
    "\n",
    "    for epoch in tqdm(range(epochs), leave=False):\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch[0], batch[1]\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(images)\n",
    "            loss = criterion(pred, labels)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "\n",
    "            total += len(pred)\n",
    "            correct += (pred == labels).sum().item()\n",
    "\n",
    "        train_acc.append(correct / total)\n",
    "        val_acc.append(validate(model, val_loader))\n",
    "        # TODO: add loss log\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{epochs}]\")\n",
    "        print(f\"Train accuracy: {train_acc[-1]:.4f}\")\n",
    "        print(f\"Val accuracy: {val_acc[-1]:.4f}\\n\")\n",
    "\n",
    "    return train_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7593f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        images, labels = batch[0], batch[1]\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(images)\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "\n",
    "        total += len(pred)\n",
    "        correct += (pred == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2230b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe0e33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(optimizer_name: str, model: ResNet, lr=0.01):\n",
    "    optimizers = {\n",
    "        \"SGD\": optim.SGD(model.parameters(), lr=lr, momentum=0.9),\n",
    "        \"Adam\": optim.Adam(model.parameters(), lr=lr),\n",
    "        \"AdamW\": optim.AdamW(model.parameters(), lr=lr),\n",
    "        \"RMSprop\": optim.RMSprop(model.parameters(), lr=lr)\n",
    "    }\n",
    "\n",
    "    return optimizers[optimizer_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20c184f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers_names = [\"SGD\", \"Adam\", \"AdamW\", \"RMSprop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb338f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [10, 20, 50, 100]\n",
    "\n",
    "blocks_num_list = [2, 2, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2810712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ResNet([2, 2, 2, 2]).to(device)\n",
    "optimizer=get_optimizer(\"SGD\", model, lr=0.001)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a6886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet18 with SGD\n",
      "10 epochs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "print(\"Training ResNet18 with SGD\")\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 10))\n",
    "\n",
    "for epoch in epochs:\n",
    "    model = ResNet(blocks_num_list).to(device)\n",
    "\n",
    "    print(\n",
    "        f\"{epoch} epochs\\n\"\n",
    "    )\n",
    "\n",
    "    train_acc, val_acc = train(\n",
    "        model,\n",
    "        criterion,\n",
    "        train_dataloaders[0],\n",
    "        val_dataloader,\n",
    "        optimizer=get_optimizer(\"SGD\", model, lr=0.001),\n",
    "        epochs=epoch,\n",
    "    )\n",
    "\n",
    "    label=f'{epoch} epochs'\n",
    "\n",
    "    ax1.plot(np.arange(1, epoch + 1), train_acc, label=label)\n",
    "    ax2.plot(np.arange(1, epoch + 1), val_acc, label=label)\n",
    "\n",
    "    test_acc = test(model, test_dataloader)\n",
    "\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True)\n",
    "\n",
    "ax1.set_title(\"SGD Training accuracy\")\n",
    "ax2.set_title(\"SGD Validation accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
